# @package training.optimizer
name: adam_atan2_pytorch
params:
  weight_decay: 1.0
  betas: [0.9, 0.95]
